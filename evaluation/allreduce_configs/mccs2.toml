# Specifiation of a Allreduce application experiment

# Number of jobs
ncases = 50

# Job size distributions [(percentage, number of workers)]
# In mccs simulation, we treat each GPU as a VM slot in nethint.
# Allocating GPUs is just like allocating VMs to the Brain (the cloud controller).
# job_size_distribution = [[40, 2], [40, 4], [40, 8], [40, 16], [40, 32]]
# job_size_distribution = [[40, 2], [40, 4], [40, 8], [40, 16], [40, 32]]
# job_size_distribution = [[40, 64]]
job_size_distribution = [[40, 16], [40, 32]]
# job_size_distribution = [[40, 2], [40, 4], [80, 32]]

# Buffer size of all jobs, in bytes, similar to ResNet50 ~= 98MB
buffer_size = 100_000_000

# Number of iterations for all jobs
num_iterations = 100

# Lambda of the poisson arrival, 2*100MB/50Gbps = 0.032s each iteration, total in 32s
# poisson_lambda = 24_000_000_000.0
# poisson_lambda = 72_000_000.0
poisson_lambda = 200_000_000.0
# poisson_lambda = 720_000_000.0

placement_strategy = { type = "Compact" }
# placement_strategy = { type = "CompactLoadBalanced" }
# placement_strategy = { type = "Spread" }
# placement_strategy = { type = "Random", args = 0 }

# global seed
seed = 1

# Output path of for the simulation results
directory = "/tmp/mccs_init/mccs2"

# Number of repeats for each batch of experiments
batch_repeat = 5

[[batch]]
policy = "Random"
probe = { enable = false }
nethint_level = 0

[[batch]]
policy = "TopologyAware"
probe = { enable = false }
nethint_level = 1
num_rings = 16

[[batch]]
policy = "Mccs"
probe = { enable = false }
nethint_level = 1
num_rings = 16

# [[batch]]
# policy = "RAT"
# probe = { enable = false }
# nethint_level = 2
# # Auto tune after some iterations. default is disabled
# auto_tune = 10

[simulator]
nethint = true
sample_interval_ns = 100_000_000 # 100ms
loopback_speed = 400
# possible values of fairness model are "PerFlowMaxMin", "PerVmPairMaxMin", and "TenantFlowMaxMin"
# fairness = "TenantFlowMaxMin"
fairness = "PerFlowMaxMin"

background_flow_hard = { enable = true, frequency_ns = 10_000_000_000, probability = 1.0, amplitude = 5 }
# background_flow_hard = { enable = false }
# nethint_delay_ms = 100

[brain]
# Random seed for reproducible result
seed = 1
# Whether the cluster's bandwidth is asymmetric
asymmetric = false
# The percentage of nodes marked broken
broken = 0.5
# The slots of each physical machine (Each slot is a GPU in mccs)
max_slots = 8
# how bandwidth is partitioned among multiple VMs in the same physical server, possible values are "RateLimited", "Guaranteed"
# sharing_mode = "Guaranteed"
sharing_mode = "RateLimited" # Every GPU gets host_bw / max_slots this amount of bandwidth
# in Gbps
# guaranteed_bandwidth = 0
background_flow_high_freq = { enable = true, probability = 1.0, amplitude = 10 }
# background_flow_high_freq = { enable = false }
gc_period = 2

# The topology for simulation
[brain.topology]
type = "TwoLayerMultiPath"  # FatTree|Arbitrary|TwoLayerMultiPath

# [brain.topology.args] # When type = "FatTree"
# nports = 20           # the number of ports of a switch
# bandwidth = 100       # in Gbps
# oversub_ratio = 4.0   # oversubscription ratio

# [brain.topology.args]         # When type = "Arbitrary"
# nracks = 16        # the number of racks
# rack_size = 16      # the number of hosts under a rack
# host_bw = 1600       # bandwidth of a host, in Gbps
# rack_bw = 25600       # bandwidth of a ToR switch, in Gbps

[brain.topology.args]    # When type = "TwoLayerMultiPath"
nspines = 16
nracks = 24
rack_size = 4
host_bw = 1600
rack_uplink_port_bw = 200
# load_balancer_type = "EcmpEverything"  # EcmpEverything|EcmpSourcePort
load_balancer_type = "EcmpSourcePort"  # EcmpEverything|EcmpSourcePort

# [envs]
# KEY = "value"
