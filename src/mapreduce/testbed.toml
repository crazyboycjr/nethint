# Specifiation of a MapReduce application experiment

# Run the experiments from trace file
trace = "/home/tenant/FB2010-1Hr-150-0.txt"

# Number of testcases to run
ncases = 20

allow_delay = true

# Number of map tasks and reduce tasks
# When running from trace, these parameters become scale factors
num_map = 1
num_reduce = 1

map_scale = 0.04
reduce_scale = 0.04

# Multiply the traffic size by a number
traffic_scale = 1.0

# Scale the time of job arrival
time_scale = 0.1

# enable computation time: false: off true: on
enable_computation_time = false

# Mapper placement policy
# mapper_policy = { type = "RandomSkew", args = [1, 0.2] }
# mapper_policy = { type = "Random", args = 1 }
mapper_policy = { type = "Greedy" }

placement_strategy = { type = "Compact" }
# placement_strategy = { type = "CompactLoadBalanced" }
# placement_strategy = { type = "Spread" }
# placement_strategy = { type = "Random", args = 0 }

# Output path of for the simulation results
directory = "/tmp/mapr_result"

# Whether to allow a mapper to collocate with a reduce
collocate = true

# shuffle = { type = "FromTrace" }
# shuffle = { type = "Uniform", args = 1000000 }
# shuffle = { type = "Zipf", args = [ 1000000, 0.1 ] }

# Number of repeats for each batch of experiments
batch_repeat = 1

[[batch]]
reducer_policy = "Random"
probe = { enable = false }
# NetHint level, possible values are 0, 1, 2
nethint_level = 1

[[batch]]
reducer_policy = "HierarchicalGreedyLevel1"
probe = { enable = true, round_ms = 10 }
nethint_level = 1

[[batch]]
reducer_policy = "HierarchicalGreedyLevel1"
probe = { enable = false }
nethint_level = 1

[[batch]]
reducer_policy = "HierarchicalGreedyPaper" # please use this
# reducer_policy = "HierarchicalGreedy"
probe = { enable = false }
nethint_level = 2

[simulator]
nethint = true
sample_interval_ns = 100_000_000 # 100ms
loopback_speed = 400
# possible values of fairness model are "PerFlowMaxMin", "PerVmPairMaxMin", and "TenantFlowMaxMin"
# fairness = "TenantFlowMaxMin"
fairness = "PerFlowMaxMin"

# emulate background flow by subtracting a bandwidth to each link
# note that the remaining bandwidth must not smaller than link_bw / current_tenants
# the amplitude range in [1, 9], it cut down the original bandwidth of a link by up to amplitude/10.
# for example, if amplitude = 9, it means that up to 90Gbps (90%) can be taken from a 100Gbps link.
background_flow_hard = { enable = true, frequency_ns = 100_000_000_000, probability = 0.9, amplitude = 5 }
# background_flow_hard = { enable = false }
# nethint_delay_ms = 100

[brain]
# Random seed for multiple uses
seed = 1
# Whether the cluster's bandwidth is asymmetric
asymmetric = false
# The percentage of nodes marked broken
broken = 0.1
# The slots of each physical machine
max_slots = 4
# how bandwidth is partitioned among multiple VMs in the same physical server, possible values are "RateLimited", "Guaranteed"
sharing_mode = "Guaranteed"
guaranteed_bandwidth = 25
background_flow_high_freq = { enable = true, probability = 0.1, amplitude = 1 }
gc_period = 100

# The topology for simulation
[brain.topology]
type = "Arbitrary"  # another possible value is "FatTree"

# [brain.topology.args] # When type = "FatTree"
# nports = 20           # the number of ports of a switch
# bandwidth = 100       # in Gbps
# oversub_ratio = 4.0   # oversubscription ratio

[brain.topology.args]         # When type = "Arbitrary"
nracks = 150        # the number of racks
rack_size = 18      # the number of hosts under a rack
host_bw = 100       # bandwidth of a host, in Gbps
rack_bw = 600       # bandwidth of a ToR switch, in Gbps

[envs]
NETHINT_PROVISION_LOG_FILE = "/tmp/mapr_result/provision.txt"
